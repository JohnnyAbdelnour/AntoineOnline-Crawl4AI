# E-commerce Product Extractor

This script uses `crawl4ai` to crawl an e-commerce website, extract structured product information, and store it in a Supabase table.

## Two-Stage Process

The script operates in two distinct modes:

1.  **`discover`**: In this mode, the script crawls the entire website to find the URLs of all product pages. It saves these URLs to a file named `product_urls.txt`.
2.  **`extract`**: In this mode, the script reads the URLs from `product_urls.txt`, visits each page, extracts the product details using an LLM, and saves the structured data to your Supabase `products` table.

This two-stage process is more robust and efficient, as it separates the discovery of product pages from the data extraction process.

## Supabase Setup

You will need a Supabase project to store the crawled data.

1.  Create a new project on [Supabase](https://supabase.com/).
2.  In your project, go to the "SQL Editor" and run the following query to create the `products` table:

```sql
CREATE TABLE "products" (
    id bigint generated by default as identity primary key,
    url text,
    name text,
    price float,
    description text,
    sku text,
    image_url text,
    created_at timestamp with time zone default now()
);
```

## Running the Crawler

1.  Clone this repository.
2.  Install the dependencies: `pip install -r requirements.txt`
3.  Create a `.env` file by copying the `.env.example` file: `cp .env.example .env`
4.  Fill in the required values in the `.env` file, including your OpenAI API key for LLM-based extraction.
5.  **Configure the `PRODUCT_URL_PATTERN`**:
    -   Before running the crawler, you need to determine the URL pattern for product pages on your target website.
    -   For example, if your product pages have URLs like `https://example.com/products/my-product-name`, your pattern would be `/products/`.
    -   Set this value for the `PRODUCT_URL_PATTERN` variable in your `.env` file.
6.  Run the `discover` mode to find all product URLs:
    ```bash
    python ecommerce_crawler.py discover
    ```
7.  Once the discovery is complete, run the `extract` mode to extract the product data:
    ```bash
    python ecommerce_crawler.py extract
    ```