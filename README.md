# E-commerce Product Extractor

This script uses `crawl4ai` to crawl an e-commerce website, extract structured product information using CSS selectors, and store it in a Supabase table.

## Two-Stage Process

The script operates in two distinct modes:

1.  **`discover`**: In this mode, the script crawls the entire website to find the URLs of all product pages. It saves these URLs to a file named `product_urls.txt`.
2.  **`extract`**: In this mode, the script reads the URLs from `product_urls.txt`, visits each page, extracts the product details using CSS selectors, and saves the structured data to your Supabase `products` table.

This two-stage process is more robust and efficient, as it separates the discovery of product pages from the data extraction process. The crawler is also configured with an increased navigation timeout and advanced URL filtering to handle complex websites and prevent errors.

## Supabase Setup

You will need a Supabase project to store the crawled data.

1.  Create a new project on [Supabase](https://supabase.com/).
2.  In your project, go to the "SQL Editor" and run the following query to create the `products` table:

```sql
CREATE TABLE "products" (
    id bigint generated by default as identity primary key,
    url text,
    name text,
    price float,
    description text,
    image_url text,
    created_at timestamp with time zone default now()
);
```

## Running the Crawler

1.  Clone this repository.
2.  Install the dependencies: `pip install -r requirements.txt`
3.  Create a `.env` file by copying the `.env.example` file: `cp .env.example .env`
4.  Fill in the required values in the `.env` file.
5.  **Configure the `PRODUCT_URL_PATTERN`**:
    -   Before running the crawler, you need to determine the URL pattern for product pages on your target website.
    -   For example, if your product pages have URLs like `https://example.com/products/my-product-name`, your pattern would be `/products/`.
    -   Set this value for the `PRODUCT_URL_PATTERN` variable in your `.env` file.
6.  **Configure the CSS Selectors**:
    -   To extract the product data, you need to provide the CSS selectors for each data field.
    -   To find the selectors, open a product page in your browser, right-click on the element you want to extract (e.g., the product name), and select "Inspect".
    -   In the developer tools, right-click on the highlighted HTML element and choose "Copy > Copy selector".
    -   Paste the copied selector into the corresponding `CSS_SELECTOR_*` variable in your `.env` file.
    -   The `CSS_SELECTOR_BASE` is the selector for the container that holds all the product information. This is useful for pages that have multiple products, as it tells the crawler where to look for each product. If you are only extracting one product per page, you can leave this as `body`.
    -   **Note:** The script now uses a list-based schema for the CSS selectors. The environment variables are used to build this schema dynamically.
7.  Run the `discover` mode to find all product URLs:
    ```bash
    python ecommerce_crawler.py discover
    ```
8.  Once the discovery is complete, run the `extract` mode to extract the product data:
    ```bash
    python ecommerce_crawler.py extract
    ```

## Running the AI Agent

After you have extracted the product data, you can run the AI agent to ask questions about it.

1.  **Set your OpenAI API Key**:
    -   Make sure you have an OpenAI account and an API key.
    -   Add your API key to the `.env` file: `OPENAI_API_KEY=your_key_here`
2.  **Run the agent**:
    ```bash
    uvicorn agent:app --reload
    ```
3.  **Open your browser**:
    -   Navigate to `http://127.0.0.1:8000` to interact with the AI agent.